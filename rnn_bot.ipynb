{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div id=d1243ebe-6151-4530-8386-2230469079e9 style=\"display:none; background-color:#9D6CFF; color:white; width:200px; height:30px; padding-left:5px; border-radius:4px; flex-direction:row; justify-content:space-around; align-items:center;\" onmouseover=\"this.style.backgroundColor='#BA9BF8'\" onmouseout=\"this.style.backgroundColor='#9D6CFF'\" onclick=\"window.commands?.execute('create-mitosheet-from-dataframe-output');\">See Full Dataframe in Mito</div> <script> if (window.commands?.hasCommand('create-mitosheet-from-dataframe-output')) document.getElementById('d1243ebe-6151-4530-8386-2230469079e9').style.display = 'flex' </script> <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-08-04 05:50:00</td>\n",
       "      <td>23780.0</td>\n",
       "      <td>23780.0</td>\n",
       "      <td>23198.0</td>\n",
       "      <td>23198.0</td>\n",
       "      <td>84.921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-08-04 05:51:00</td>\n",
       "      <td>23228.2</td>\n",
       "      <td>23900.0</td>\n",
       "      <td>23198.0</td>\n",
       "      <td>23900.0</td>\n",
       "      <td>84.941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-08-04 05:52:00</td>\n",
       "      <td>23852.2</td>\n",
       "      <td>23898.0</td>\n",
       "      <td>23198.0</td>\n",
       "      <td>23898.0</td>\n",
       "      <td>132.174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-08-04 05:53:00</td>\n",
       "      <td>23898.0</td>\n",
       "      <td>23900.0</td>\n",
       "      <td>23198.0</td>\n",
       "      <td>23228.2</td>\n",
       "      <td>74.257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-08-04 05:54:00</td>\n",
       "      <td>23520.4</td>\n",
       "      <td>23946.0</td>\n",
       "      <td>23198.0</td>\n",
       "      <td>23653.4</td>\n",
       "      <td>83.717</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div>"
      ],
      "text/plain": [
       "                  Date     Open     High      Low    Close   Volume\n",
       "0  2022-08-04 05:50:00  23780.0  23780.0  23198.0  23198.0   84.921\n",
       "1  2022-08-04 05:51:00  23228.2  23900.0  23198.0  23900.0   84.941\n",
       "2  2022-08-04 05:52:00  23852.2  23898.0  23198.0  23898.0  132.174\n",
       "3  2022-08-04 05:53:00  23898.0  23900.0  23198.0  23228.2   74.257\n",
       "4  2022-08-04 05:54:00  23520.4  23946.0  23198.0  23653.4   83.717"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"out.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data\n",
    "\n",
    "Here we take the data from source (in this case csv), then add/remove columns we need in order for the model to properly work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Close    Volume  future_close\n",
      "0 -0.703865  0.058691      1.270997\n",
      "1  1.270997  0.058971      1.265370\n",
      "2  1.265370  0.720967     -0.618907\n",
      "3 -0.618907 -0.090771      0.577263\n",
      "4  0.577263  0.041816     -0.703865\n"
     ]
    }
   ],
   "source": [
    "import helpers.process_data as process_data\n",
    "\n",
    "# Initiate scaler\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "## process data\n",
    "\n",
    "# number of rows in the future\n",
    "# in this case we want to predict 1 single row into the future (1 minute)\n",
    "FUTURE_PRICE_ROWS=1\n",
    "\n",
    "data = process_data.process_df(df, future_n_rows=FUTURE_PRICE_ROWS, scaler=scaler)\n",
    "print(data.head())\n",
    "\n",
    "# Change dataframe data to numpy array\n",
    "data = data.to_numpy()\n",
    "\n",
    "# Inverse scale\n",
    "# print(scaler.feature_names_in_[0])\n",
    "# print(data[0,:2])\n",
    "# print(scaler.inverse_transform([data[0,:2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split and load data\n",
    "\n",
    "Split data and load them into dataloaders.\n",
    "\n",
    "Dataloaders are a great way to abstract and handle data for training process, this way you don't have to work with matrices, but objects (dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9500, 3)\n",
      "(500, 3)\n"
     ]
    }
   ],
   "source": [
    "# split data\n",
    "train_data, test_data = process_data.split_data(data, test_percent=5)\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and DataLoader\n",
    "from helpers.dataloaders import StockDataSet\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataset = StockDataSet(train_data)\n",
    "test_dataset = StockDataSet(test_data)\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "test_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up model\n",
    "\n",
    "Here we instantiate the model. In this case we use an LSTM RNN model since it has better long-range dependency between each step in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (rnn): LSTM(2, 64, batch_first=True)\n",
       "  (linear): Linear(in_features=3840, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import models.recursive_nets as model\n",
    "\n",
    "\"\"\"\n",
    "variables explanation for model instantiation\n",
    "\n",
    "- 2 input vectors\n",
    "- 64 hidden neurons\n",
    "- 60 steps for each sequence\n",
    "- 1 single output vector\n",
    "\"\"\"\n",
    "\n",
    "RNNModel = model.LSTM(2, 64, 60, 1)\n",
    "RNNModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate accuracy function\n",
    "\n",
    "This function is in charge of calculating the accuracy for the regression model. It goes through all the items in the provided dataloader. it returns MAPE (mean absolute percentage error) which is basically the error percentage. The idea is that this function returns lower numbers on each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1692607402801514"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_accuracy_error(model: nn.Module, data: DataLoader, device: str) -> float:\n",
    "    \"\"\"Returns MAPE accuracy error percentage for the given dataloader complete run\n",
    "    \n",
    "    Keyword arguments:\n",
    "    model: pytorch module\n",
    "    data: dataloader to iterate and calculate MAPE\n",
    "\n",
    "    Return:\n",
    "    MAPE: Error percentage\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    APE = list()\n",
    "\n",
    "    for batch, (X, Y) in enumerate(data):\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(X)\n",
    "            out = out.squeeze(dim=0)\n",
    "            APE.append(torch.mean(torch.abs((Y - out) / Y)))\n",
    "\n",
    "    MAPE = torch.mean(torch.Tensor(APE)).detach().item()\n",
    "    return MAPE\n",
    "\n",
    "# sample\n",
    "calculate_accuracy_error(RNNModel, test_dataloader, \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop.\n",
    "\n",
    "This holds the main logic to iterate through each batch on the dataloader and run the forward and backward pass on the model so it learns\n",
    "\n",
    "Training data is uploaded to [wandb.ai](https://wandb.ai/). In there you can visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb as wdb\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "# Training Loop\n",
    "def train(model: nn.Module,\n",
    "    train_data: DataLoader,\n",
    "    test_data: DataLoader,\n",
    "    optimizer: optim.Optimizer,\n",
    "    loss_fn: nn.Module,\n",
    "    epochs: int = 10,\n",
    "    enable_wandb: bool = False) -> None:\n",
    "    \"\"\"\n",
    "    Trains the RNN model for the specified number of epochs\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    model: RNN model to train (should inherit from nn.Module)\n",
    "    train_data: Iterable DataLoader\n",
    "    test_data: Iterable DataLoader\n",
    "    epochs: Number of epochs to train the model\n",
    "    optiimizer: Optimizer function to use for each epoch\n",
    "    loss_fn: Loss function to use\n",
    "    \"\"\"\n",
    "    if enable_wandb:\n",
    "        wdb.init(project=\"trading-bot\")\n",
    "\n",
    "    train_losses = list()\n",
    "    train_accuracy_error = list()\n",
    "    test_accuracy_error = list()\n",
    "    model.to(device)\n",
    "\n",
    "    with tqdm(range(epochs), unit=\"epoch\") as t:\n",
    "            for epoch in t:\n",
    "                model.train()\n",
    "                epoch_losses = list()\n",
    "                for X, Y in train_data:\n",
    "                    # # skip batch if it doesnt match with the batch_size\n",
    "                    # if X.shape[0] != model.batch_size:\n",
    "                    #     continue\n",
    "\n",
    "                    # send tensors to device\n",
    "                    X, Y = X.to(device), Y.to(device)\n",
    "\n",
    "                    # 2. clear gradients\n",
    "                    model.zero_grad()\n",
    "\n",
    "                    loss = 0\n",
    "                    # Internal loop for RNN to train on each step of seq_length\n",
    "                    in_vector = X\n",
    "                    out = model(in_vector)\n",
    "                    l = loss_fn(out.squeeze(dim=0), Y)\n",
    "                    # print(f'in: {in_vector}')\n",
    "                    # print(f'{out}, res: {Y[:, c]}')\n",
    "\n",
    "                    loss += l\n",
    "                    # print(loss.detach().item() / X.shape[1])\n",
    "\n",
    "                    # 4. Compte gradients gradients\n",
    "                    loss.backward()\n",
    "\n",
    "                    # 5. Adjust learnable parameters\n",
    "                    # clip as well to avoid vanishing and exploding gradients\n",
    "                    nn.utils.clip_grad_norm_(model.parameters(), 3)\n",
    "                    optimizer.step()\n",
    "                    epoch_losses.append(loss.detach().item() / X.shape[0])\n",
    "                train_losses.append(torch.tensor(epoch_losses).mean())\n",
    "                train_accuracy_error.append(calculate_accuracy_error(RNNModel, train_data, device))\n",
    "                test_accuracy_error.append(calculate_accuracy_error(RNNModel, test_data, device))\n",
    "                # print(f'=> epoch: {epoch + 1}, loss: {train_losses[epoch]}')\n",
    "                if enable_wandb:\n",
    "                    wdb.log({'loss': train_losses[-1], \n",
    "                            'train_accuracy_error': train_accuracy_error[-1],\n",
    "                            'test_accuracy_error': test_accuracy_error[-1]})\n",
    "                t.set_postfix(train_losses=f\"{train_losses[-1]:>5f}\", \n",
    "                    train_acc=f\"{train_accuracy_error[-1]:.2f}%\",\n",
    "                    test_acc=f\"{test_accuracy_error[-1]:.2f}%\",\n",
    "                )\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "\n",
    "Since this is a regression problem we are using MSE as the loss function, and Adam for the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:31oq7n5g) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7c23fbc3e08489b8d23113a5550eac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▆▅▅▅▄▄▃▃▃▃▂▁▁▁▂▁▁</td></tr><tr><td>test_accuracy_error</td><td>▆▄▄▄▃▁▁▁▁▄▄▃▅█▅▆▄▄</td></tr><tr><td>train_accuracy_error</td><td>▆▄▄▄▃▁▁▁▁▄▄▃▅█▅▆▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.07638</td></tr><tr><td>test_accuracy_error</td><td>3.25018</td></tr><tr><td>train_accuracy_error</td><td>3.25018</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">crimson-star-25</strong>: <a href=\"https://wandb.ai/versus/trading-bot/runs/31oq7n5g\" target=\"_blank\">https://wandb.ai/versus/trading-bot/runs/31oq7n5g</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220822_182436-31oq7n5g/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:31oq7n5g). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/diego/Documents/Projects/trading-bot/wandb/run-20220822_182503-4pv8xjzv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/versus/trading-bot/runs/4pv8xjzv\" target=\"_blank\">crimson-spaceship-26</a></strong> to <a href=\"https://wandb.ai/versus/trading-bot\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "329a609601f04cf8b0b17362b71c7a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diego/.local/share/virtualenvs/trading-bot-Nm060_Ai/lib/python3.10/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.Adam(RNNModel.parameters(), lr=1e-2)\n",
    "train(RNNModel, train_dataloader, test_dataloader, optimizer, loss_function, 10, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "Function to evaluate a single sequence. This just accepts a single sequence and returns the predicted value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23731.875541566627"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "# sample for getting a single item from a dataset object. \n",
    "# No need for a dataloader here since it is a single input/output vector \n",
    "# (tuple[tensor, tensor])\n",
    "# train_dataset[0]\n",
    "\n",
    "def eval_model(\n",
    "        model: nn.Module, \n",
    "        sequence: tuple[torch.Tensor, torch.Tensor],\n",
    "        scaler: TransformerMixin | None = None\n",
    "    ) -> float:\n",
    "    \"\"\"eval model with a single row input tensor\n",
    "    \n",
    "    Keyword arguments:\n",
    "    model: RNN model\n",
    "    sequence: complete input sequence (seq_length x out_features)\n",
    "    scaler (Optional): Optional scaler to return scaled value\n",
    "\n",
    "    Return: Predicted value (could be scaled if scaler is provided) \n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    # add 1 more dimension to simulate a batch\n",
    "    X = sequence[0].unsqueeze(0)\n",
    "    Y = sequence[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(X)\n",
    "\n",
    "    if isinstance(scaler, TransformerMixin):\n",
    "        zero_tensor = torch.Tensor([[out, 0]])\n",
    "        out_scaled = scaler.inverse_transform(zero_tensor)\n",
    "        return out_scaled.squeeze()[0].item()\n",
    "\n",
    "    return out.detach().item()\n",
    "\n",
    "\n",
    "eval_model(RNNModel, test_dataset[0], scaler)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('trading-bot-Nm060_Ai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "096b8751dc166c8f95cca38682078aae5dd055a535232894d7ae01156c8afa28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
