{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import helpers.functions as helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div id=765d1f70-f8c4-474e-8f50-f21790f45436 style=\"display:none; background-color:#9D6CFF; color:white; width:200px; height:30px; padding-left:5px; border-radius:4px; flex-direction:row; justify-content:space-around; align-items:center;\" onmouseover=\"this.style.backgroundColor='#BA9BF8'\" onmouseout=\"this.style.backgroundColor='#9D6CFF'\" onclick=\"window.commands?.execute('create-mitosheet-from-dataframe-output');\">See Full Dataframe in Mito</div> <script> if (window.commands?.hasCommand('create-mitosheet-from-dataframe-output')) document.getElementById('765d1f70-f8c4-474e-8f50-f21790f45436').style.display = 'flex' </script> <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-08-04 05:50:00</td>\n",
       "      <td>23780.0</td>\n",
       "      <td>23780.0</td>\n",
       "      <td>23198.0</td>\n",
       "      <td>23198.0</td>\n",
       "      <td>84.921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-08-04 05:51:00</td>\n",
       "      <td>23228.2</td>\n",
       "      <td>23900.0</td>\n",
       "      <td>23198.0</td>\n",
       "      <td>23900.0</td>\n",
       "      <td>84.941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-08-04 05:52:00</td>\n",
       "      <td>23852.2</td>\n",
       "      <td>23898.0</td>\n",
       "      <td>23198.0</td>\n",
       "      <td>23898.0</td>\n",
       "      <td>132.174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-08-04 05:53:00</td>\n",
       "      <td>23898.0</td>\n",
       "      <td>23900.0</td>\n",
       "      <td>23198.0</td>\n",
       "      <td>23228.2</td>\n",
       "      <td>74.257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-08-04 05:54:00</td>\n",
       "      <td>23520.4</td>\n",
       "      <td>23946.0</td>\n",
       "      <td>23198.0</td>\n",
       "      <td>23653.4</td>\n",
       "      <td>83.717</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div>"
      ],
      "text/plain": [
       "                  Date     Open     High      Low    Close   Volume\n",
       "0  2022-08-04 05:50:00  23780.0  23780.0  23198.0  23198.0   84.921\n",
       "1  2022-08-04 05:51:00  23228.2  23900.0  23198.0  23900.0   84.941\n",
       "2  2022-08-04 05:52:00  23852.2  23898.0  23198.0  23898.0  132.174\n",
       "3  2022-08-04 05:53:00  23898.0  23900.0  23198.0  23228.2   74.257\n",
       "4  2022-08-04 05:54:00  23520.4  23946.0  23198.0  23653.4   83.717"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"out.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data\n",
    "\n",
    "Here we take the data from source (in this case csv), then add/remove columns we need in order for the model to properly work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "## preprocess\n",
    "\n",
    "# number of rows in the future\n",
    "# in this case we want to predict 5 minutes into the future\n",
    "FUTURE_PRICE_ROWS=1\n",
    "\n",
    "data = helpers.preprocess_df(df, future_n_rows=FUTURE_PRICE_ROWS, scaler=scaler)\n",
    "data.head()\n",
    "data = data.to_numpy()\n",
    "\n",
    "# Inverse scale\n",
    "# print(data[0,:2])\n",
    "# print(scaler.inverse_transform([data[0,:2]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split and load data\n",
    "\n",
    "Split data and load them into dataloaders.\n",
    "\n",
    "Dataloaders are a great way to abstract and handle data for training process, this way you don't have to work with matrices, but objects (dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9500, 3)\n",
      "(500, 3)\n"
     ]
    }
   ],
   "source": [
    "# split data\n",
    "train_data, test_data = helpers.split_np_matrix(data, test_percent=5)\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataloader\n",
    "from helpers.dataloaders import StockDataloader\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataset = StockDataloader(train_data)\n",
    "test_dataset = StockDataloader(test_data)\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "test_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up model and training loop\n",
    "\n",
    "Here we instantiate the model. In this case we use a simple RNN model, but we can do any sorts of combinations we want\n",
    "\n",
    "We also set up the training process. This will be the same most of the time, so its easier to just to use a library for this to handle for us\n",
    "\n",
    "Training data is uploaded to [wandb.ai](https://wandb.ai/). In there you can visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (rnn): LSTM(2, 64, batch_first=True)\n",
       "  (linear): Linear(in_features=3840, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import models.recursive_nets as model\n",
    "\n",
    "RNNModel = model.LSTM(2, 64, 60, 1)\n",
    "X, Y = next(iter(train_dataloader))\n",
    "RNNModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchest.trainer import Trainer\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class RNNTrainer(Trainer):\n",
    "    def train(self, \n",
    "        data_train: DataLoader,\n",
    "        data_dev: DataLoader | None = None,\n",
    "        data_test: DataLoader | None = None,\n",
    "        epochs: int = 100,\n",
    "        early_stoping: float = 0.0\n",
    "    ) -> None:\n",
    "        self.model.to(device)\n",
    "        with tqdm(range(epochs), unit=\"epoch\") as t:\n",
    "            for epoch in t:\n",
    "                self.model.train()\n",
    "                train_losses = list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "\n",
    "# Training Loop\n",
    "def train(model: nn.Module,\n",
    "    data: DataLoader,\n",
    "    optimizer: optim.Optimizer,\n",
    "    loss_fn: nn.Module,\n",
    "    epochs: int = 10) -> None:\n",
    "    \"\"\"\n",
    "    Trains the RNN model for the specified number of epochs\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    model: RNN model to train (should inherit from nn.Module)\n",
    "    data: Iterable DataLoader\n",
    "    epochs: Number of epochs to train the model\n",
    "    optiimizer: Optimizer function to use for each epoch\n",
    "    loss_fn: Loss function to use\n",
    "    \"\"\"\n",
    "    train_losses = {}\n",
    "    model.to(device)\n",
    "\n",
    "    with tqdm(range(epochs), unit=\"epoch\") as t:\n",
    "            for epoch in t:\n",
    "                model.train()\n",
    "                epoch_losses = list()\n",
    "                for X, Y in data:\n",
    "                    # # skip batch if it doesnt match with the batch_size\n",
    "                    # if X.shape[0] != model.batch_size:\n",
    "                    #     continue\n",
    "\n",
    "                    # send tensors to device\n",
    "                    X, Y = X.to(device), Y.to(device)\n",
    "\n",
    "                    # 2. clear gradients\n",
    "                    model.zero_grad()\n",
    "\n",
    "                    loss = 0\n",
    "                    # Internal loop for RNN to train on each step of seq_length\n",
    "                    in_vector = X\n",
    "                    out = model(in_vector)\n",
    "                    l = loss_fn(out, Y[:, -1])\n",
    "                    # print(f'in: {in_vector}')\n",
    "                    # print(f'{out}, res: {Y[:, c]}')\n",
    "                    loss += l\n",
    "                    # print(loss.detach().item() / X.shape[1])\n",
    "\n",
    "                    # 4. Compte gradients gradients\n",
    "                    loss.backward()\n",
    "\n",
    "                    # 5. Adjust learnable parameters\n",
    "                    # clip as well to avoid vanishing and exploding gradients\n",
    "                    nn.utils.clip_grad_norm_(model.parameters(), 3)\n",
    "                    optimizer.step()\n",
    "                \n",
    "                    epoch_losses.append(loss.detach().item() / X.shape[1])\n",
    "\n",
    "                train_losses[epoch] = torch.tensor(epoch_losses).mean()\n",
    "                print(f'=> epoch: {epoch + 1}, loss: {train_losses[epoch]}')\n",
    "                # print(generate_text(model, data.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "333d9ac0d3cb4b17ad1fa4ee12bf9b10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0590, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.1346, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.2560, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5145, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7653, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0139, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.6274, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8232, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2226, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2685, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0135, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1600, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.5354, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6457, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.4695, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.7715, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3488, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.6651, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.1716, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.6560, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0213, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0522, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1353, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3727, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1936, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0149, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.5543, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2091, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.9365, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7573, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1092, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0074, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4682, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0434, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0326, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.1520, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.4781, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.5550, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0688, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4544, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6806, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0361, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2904, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0065, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3436, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0106, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2462, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4450, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1609, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0051, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2279, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1278, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0987, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1930, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3708, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1476, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0839, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0591, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0430, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0061, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6704, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0144, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0489, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4554, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3682, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4110, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0617, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0196, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.4059e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0681, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.8726, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0091, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2638, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7037, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1944, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0075, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0346, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2910, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3374, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0931, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3218, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.5526, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6359, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1025, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5586, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0074, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1308, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2830, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0655, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1325, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7685, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1040, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.7557, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3810, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1324, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1155, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0413, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2561, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0734, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0369, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.8158e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6653, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0916, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2333, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0146, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0366, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5329, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2090, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0296, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0863, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1045, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0103, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0038, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0509, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3491, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5800, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0931, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3568, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1480, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1160, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2720, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0360, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1306, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8969, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0674, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0125, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0098, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0102, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0935, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9787, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1113, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4903, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0065, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5289, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1269, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0199, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2159, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0953, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3952, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2954, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0198, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3283, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0953, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.1940, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0811, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1670, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3046, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3000, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3441, grad_fn=<MseLossBackward0>)\n",
      "=> epoch: 1, loss: 0.010727006010711193\n",
      "tensor(1.3419, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.9798, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0323, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0403, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4108, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3155, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0109, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3067, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2414, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2962, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1168, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0146, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3530, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.5966, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.4249, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0321, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0130, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7888, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7323, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0070, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4007, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2054, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0288, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0048, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2938, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2898, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2754, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4236, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1626, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1399, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0046, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5134, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1174, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9803, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4466, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3899, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1800, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1505, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0126, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1645, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1100, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4294, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0312, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0111, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4019, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0042, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.7669e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1270, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1057, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1134, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0610, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0154, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.8990e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1291, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0556, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0496, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0525, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0327, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1101, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2010, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0738, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0465, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3987, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2387, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1021, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3485, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0139, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0464, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0361, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.7658, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0467, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0063, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4672, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1004, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0146, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0240, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0841, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4693, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3571, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6024, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4397, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5209, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4588, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0176, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0513, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4323, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0097, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2408, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5332, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0967, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.9077, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3595, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1081, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2039, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2120, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6385, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7029, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1338, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9396, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.5180, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0684, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.9022, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4262, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1632, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0043, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1238, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2139, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2618, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1003, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0819, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0077, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0921, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0933, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1313, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5531, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0804, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0089, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0303, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1208, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0740, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0091, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2754, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0332, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0097, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0819, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0261, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2105, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2179, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0379, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1724, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2282, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1181, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1147, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.2080e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0547, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2345, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1765, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3188, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0105, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3047, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0701, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3660, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0815, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1035, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4046, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0562, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1097, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1776, grad_fn=<MseLossBackward0>)\n",
      "=> epoch: 2, loss: 0.006008738186210394\n",
      "tensor(1.5414, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.1541, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1110, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0092, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4956, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5599, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8017, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.3946, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0773, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0130, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8066, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1262, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3878, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/diego/Documents/Projects/trading-bot/rnn_bot.ipynb Celda 13\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/diego/Documents/Projects/trading-bot/rnn_bot.ipynb#ch0000012?line=0'>1</a>\u001b[0m loss_function \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mMSELoss()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/diego/Documents/Projects/trading-bot/rnn_bot.ipynb#ch0000012?line=1'>2</a>\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(RNNModel\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m1e-2\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/diego/Documents/Projects/trading-bot/rnn_bot.ipynb#ch0000012?line=3'>4</a>\u001b[0m train(RNNModel, train_dataloader, optimizer, loss_function, \u001b[39m20\u001b[39;49m)\n",
      "\u001b[1;32m/Users/diego/Documents/Projects/trading-bot/rnn_bot.ipynb Celda 13\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data, optimizer, loss_fn, epochs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/diego/Documents/Projects/trading-bot/rnn_bot.ipynb#ch0000012?line=40'>41</a>\u001b[0m \u001b[39m# Internal loop for RNN to train on each step of seq_length\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/diego/Documents/Projects/trading-bot/rnn_bot.ipynb#ch0000012?line=41'>42</a>\u001b[0m in_vector \u001b[39m=\u001b[39m X\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/diego/Documents/Projects/trading-bot/rnn_bot.ipynb#ch0000012?line=42'>43</a>\u001b[0m out \u001b[39m=\u001b[39m model(in_vector)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/diego/Documents/Projects/trading-bot/rnn_bot.ipynb#ch0000012?line=43'>44</a>\u001b[0m l \u001b[39m=\u001b[39m loss_fn(out, Y[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/diego/Documents/Projects/trading-bot/rnn_bot.ipynb#ch0000012?line=44'>45</a>\u001b[0m \u001b[39mprint\u001b[39m(l)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/trading-bot-Nm060_Ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Projects/trading-bot/models/recursive_nets.py:47\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m c0 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minit_zero_hidden(x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mto(x\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     46\u001b[0m \u001b[39m# Pass through LSTM\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m out, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnn(x, (h0, c0))\n\u001b[1;32m     49\u001b[0m \u001b[39m# reshape to flatten hidden_size * seq_length\u001b[39;00m\n\u001b[1;32m     50\u001b[0m out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mreshape(out\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/trading-bot-Nm060_Ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/trading-bot-Nm060_Ai/lib/python3.10/site-packages/torch/nn/modules/rnn.py:769\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    768\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m    770\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[1;32m    771\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    773\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.Adam(RNNModel.parameters(), lr=1e-2)\n",
    "\n",
    "train(RNNModel, train_dataloader, optimizer, loss_function, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('trading-bot-Nm060_Ai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "096b8751dc166c8f95cca38682078aae5dd055a535232894d7ae01156c8afa28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
