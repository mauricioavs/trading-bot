{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import helpers.functions as helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div id=765d1f70-f8c4-474e-8f50-f21790f45436 style=\"display:none; background-color:#9D6CFF; color:white; width:200px; height:30px; padding-left:5px; border-radius:4px; flex-direction:row; justify-content:space-around; align-items:center;\" onmouseover=\"this.style.backgroundColor='#BA9BF8'\" onmouseout=\"this.style.backgroundColor='#9D6CFF'\" onclick=\"window.commands?.execute('create-mitosheet-from-dataframe-output');\">See Full Dataframe in Mito</div> <script> if (window.commands?.hasCommand('create-mitosheet-from-dataframe-output')) document.getElementById('765d1f70-f8c4-474e-8f50-f21790f45436').style.display = 'flex' </script> <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-08-04 05:50:00</td>\n",
       "      <td>23780.0</td>\n",
       "      <td>23780.0</td>\n",
       "      <td>23198.0</td>\n",
       "      <td>23198.0</td>\n",
       "      <td>84.921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-08-04 05:51:00</td>\n",
       "      <td>23228.2</td>\n",
       "      <td>23900.0</td>\n",
       "      <td>23198.0</td>\n",
       "      <td>23900.0</td>\n",
       "      <td>84.941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-08-04 05:52:00</td>\n",
       "      <td>23852.2</td>\n",
       "      <td>23898.0</td>\n",
       "      <td>23198.0</td>\n",
       "      <td>23898.0</td>\n",
       "      <td>132.174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-08-04 05:53:00</td>\n",
       "      <td>23898.0</td>\n",
       "      <td>23900.0</td>\n",
       "      <td>23198.0</td>\n",
       "      <td>23228.2</td>\n",
       "      <td>74.257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-08-04 05:54:00</td>\n",
       "      <td>23520.4</td>\n",
       "      <td>23946.0</td>\n",
       "      <td>23198.0</td>\n",
       "      <td>23653.4</td>\n",
       "      <td>83.717</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div>"
      ],
      "text/plain": [
       "                  Date     Open     High      Low    Close   Volume\n",
       "0  2022-08-04 05:50:00  23780.0  23780.0  23198.0  23198.0   84.921\n",
       "1  2022-08-04 05:51:00  23228.2  23900.0  23198.0  23900.0   84.941\n",
       "2  2022-08-04 05:52:00  23852.2  23898.0  23198.0  23898.0  132.174\n",
       "3  2022-08-04 05:53:00  23898.0  23900.0  23198.0  23228.2   74.257\n",
       "4  2022-08-04 05:54:00  23520.4  23946.0  23198.0  23653.4   83.717"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"out.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data\n",
    "\n",
    "Here we take the data from source (in this case csv), then add/remove columns we need in order for the model to properly work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "## preprocess\n",
    "\n",
    "# number of rows in the future\n",
    "# in this case we want to predict 5 minutes into the future\n",
    "FUTURE_PRICE_ROWS=1\n",
    "\n",
    "data = helpers.preprocess_df(df, future_n_rows=FUTURE_PRICE_ROWS, scaler=scaler)\n",
    "data.head()\n",
    "data = data.to_numpy()\n",
    "\n",
    "# Inverse scale\n",
    "# print(data[0,:2])\n",
    "# print(scaler.inverse_transform([data[0,:2]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split and load data\n",
    "\n",
    "Split data and load them into dataloaders.\n",
    "\n",
    "Dataloaders are a great way to abstract and handle data for training process, this way you don't have to work with matrices, but objects (dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9500, 3)\n",
      "(500, 3)\n"
     ]
    }
   ],
   "source": [
    "# split data\n",
    "train_data, test_data = helpers.split_np_matrix(data, test_percent=5)\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataloader\n",
    "from helpers.dataloaders import StockDataloader\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataset = StockDataloader(train_data)\n",
    "test_dataset = StockDataloader(test_data)\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "test_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up model and training loop\n",
    "\n",
    "Here we instantiate the model. In this case we use a simple RNN model, but we can do any sorts of combinations we want\n",
    "\n",
    "We also set up the training process. This will be the same most of the time, so its easier to just to use a library for this to handle for us\n",
    "\n",
    "Training data is uploaded to [wandb.ai](https://wandb.ai/). In there you can visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (rnn): LSTM(2, 64, batch_first=True)\n",
       "  (linear): Linear(in_features=3840, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import models.recursive_nets as model\n",
    "\n",
    "RNNModel = model.LSTM(2, 64, 60, 1)\n",
    "X, Y = next(iter(train_dataloader))\n",
    "RNNModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchest.trainer import Trainer\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class RNNTrainer(Trainer):\n",
    "    def train(self, \n",
    "        data_train: DataLoader,\n",
    "        data_dev: DataLoader | None = None,\n",
    "        data_test: DataLoader | None = None,\n",
    "        epochs: int = 100,\n",
    "        early_stoping: float = 0.0\n",
    "    ) -> None:\n",
    "        self.model.to(device)\n",
    "        with tqdm(range(epochs), unit=\"epoch\") as t:\n",
    "            for epoch in t:\n",
    "                self.model.train()\n",
    "                train_losses = list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "\n",
    "# Training Loop\n",
    "def train(model: nn.Module,\n",
    "    data: DataLoader,\n",
    "    optimizer: optim.Optimizer,\n",
    "    loss_fn: nn.Module,\n",
    "    epochs: int = 10) -> None:\n",
    "    \"\"\"\n",
    "    Trains the RNN model for the specified number of epochs\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    model: RNN model to train (should inherit from nn.Module)\n",
    "    data: Iterable DataLoader\n",
    "    epochs: Number of epochs to train the model\n",
    "    optiimizer: Optimizer function to use for each epoch\n",
    "    loss_fn: Loss function to use\n",
    "    \"\"\"\n",
    "    train_losses = {}\n",
    "    model.to(device)\n",
    "\n",
    "    with tqdm(range(epochs), unit=\"epoch\") as t:\n",
    "            for epoch in t:\n",
    "                model.train()\n",
    "                epoch_losses = list()\n",
    "                for X, Y in data:\n",
    "                    # # skip batch if it doesnt match with the batch_size\n",
    "                    # if X.shape[0] != model.batch_size:\n",
    "                    #     continue\n",
    "\n",
    "                    # send tensors to device\n",
    "                    X, Y = X.to(device), Y.to(device)\n",
    "\n",
    "                    # 2. clear gradients\n",
    "                    model.zero_grad()\n",
    "\n",
    "                    loss = 0\n",
    "                    # Internal loop for RNN to train on each step of seq_length\n",
    "                    in_vector = X\n",
    "                    out = model(in_vector)\n",
    "                    l = loss_fn(out, Y[:, -1])\n",
    "                    # print(f'in: {in_vector}')\n",
    "                    # print(f'{out}, res: {Y[:, c]}')\n",
    "                    loss += l\n",
    "                    # print(loss.detach().item() / X.shape[1])\n",
    "\n",
    "                    # 4. Compte gradients gradients\n",
    "                    loss.backward()\n",
    "\n",
    "                    # 5. Adjust learnable parameters\n",
    "                    # clip as well to avoid vanishing and exploding gradients\n",
    "                    nn.utils.clip_grad_norm_(model.parameters(), 3)\n",
    "                    optimizer.step()\n",
    "                \n",
    "                    epoch_losses.append(loss.detach().item() / X.shape[1])\n",
    "\n",
    "                train_losses[epoch] = torch.tensor(epoch_losses).mean()\n",
    "                print(f'=> epoch: {epoch + 1}, loss: {train_losses[epoch]}')\n",
    "                # print(generate_text(model, data.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d6f1e3b0cf741b2a6d27e1644761971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> epoch: 1, loss: 0.0059264847077429295\n",
      "=> epoch: 2, loss: 0.00479606119915843\n",
      "=> epoch: 3, loss: 0.00363823096267879\n",
      "=> epoch: 4, loss: 0.003337956266477704\n",
      "=> epoch: 5, loss: 0.0027979493606835604\n",
      "=> epoch: 6, loss: 0.0020938804373145103\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/diego/Documents/Projects/trading-bot/rnn_bot.ipynb Celda 13\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/diego/Documents/Projects/trading-bot/rnn_bot.ipynb#ch0000012?line=0'>1</a>\u001b[0m loss_function \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mMSELoss()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/diego/Documents/Projects/trading-bot/rnn_bot.ipynb#ch0000012?line=1'>2</a>\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(RNNModel\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m1e-2\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/diego/Documents/Projects/trading-bot/rnn_bot.ipynb#ch0000012?line=3'>4</a>\u001b[0m train(RNNModel, train_dataloader, optimizer, loss_function, \u001b[39m20\u001b[39;49m)\n",
      "\u001b[1;32m/Users/diego/Documents/Projects/trading-bot/rnn_bot.ipynb Celda 13\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data, optimizer, loss_fn, epochs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/diego/Documents/Projects/trading-bot/rnn_bot.ipynb#ch0000012?line=46'>47</a>\u001b[0m loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m l\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/diego/Documents/Projects/trading-bot/rnn_bot.ipynb#ch0000012?line=47'>48</a>\u001b[0m \u001b[39m# print(loss.detach().item() / X.shape[1])\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/diego/Documents/Projects/trading-bot/rnn_bot.ipynb#ch0000012?line=48'>49</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/diego/Documents/Projects/trading-bot/rnn_bot.ipynb#ch0000012?line=49'>50</a>\u001b[0m \u001b[39m# 4. Compte gradients gradients\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/diego/Documents/Projects/trading-bot/rnn_bot.ipynb#ch0000012?line=50'>51</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/diego/Documents/Projects/trading-bot/rnn_bot.ipynb#ch0000012?line=52'>53</a>\u001b[0m \u001b[39m# 5. Adjust learnable parameters\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/diego/Documents/Projects/trading-bot/rnn_bot.ipynb#ch0000012?line=53'>54</a>\u001b[0m \u001b[39m# clip as well to avoid vanishing and exploding gradients\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/diego/Documents/Projects/trading-bot/rnn_bot.ipynb#ch0000012?line=54'>55</a>\u001b[0m nn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(model\u001b[39m.\u001b[39mparameters(), \u001b[39m3\u001b[39m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/trading-bot-Nm060_Ai/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/trading-bot-Nm060_Ai/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.Adam(RNNModel.parameters(), lr=1e-2)\n",
    "\n",
    "train(RNNModel, train_dataloader, optimizer, loss_function, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('trading-bot-Nm060_Ai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "096b8751dc166c8f95cca38682078aae5dd055a535232894d7ae01156c8afa28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
